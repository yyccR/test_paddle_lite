#include <iostream>
#include "paddle_api.h"

using namespace paddle::lite_api;

int64_t ShapeProduction(const shape_t& shape) {
    int64_t res = 1;
    for (auto i : shape) res *= i;
    return res;
}

int main() {
    std::string model_file("/Users/yang/CLionProjects/test_paddle_lite/mobile_net/mobilenet_v1_opt.nb");
    // 1. Set MobileConfig
    MobileConfig config;
    // 2. Set the path to the model generated by opt tools
    config.set_model_from_file(model_file);
    // 3. Create PaddlePredictor by MobileConfig
    std::shared_ptr<PaddlePredictor> predictor = CreatePaddlePredictor<MobileConfig>(config);

    std::unique_ptr<Tensor> input_tensor(std::move(predictor->GetInput(0)));
    input_tensor->Resize({1, 3, 224, 224});
    auto* data = input_tensor->mutable_data<float>();
    for (int i = 0; i < ShapeProduction(input_tensor->shape()); ++i) {
        data[i] = 1;
    }

    predictor->Run();
    std::unique_ptr<const Tensor> output_tensor(
            std::move(predictor->GetOutput(0)));
    // 转化为数据
    const float* output_data=output_tensor->data<float>();


    std::cout << "Hello, World!" << output_data[1] << std::endl;
    return 0;
}
